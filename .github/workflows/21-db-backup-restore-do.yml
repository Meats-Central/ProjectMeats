# =============================================================================
# DATABASE BACKUP AND RESTORE USING DIGITALOCEAN API
# =============================================================================
# This workflow uses DigitalOcean's managed backup features
# =============================================================================

name: 21-Weekly DB Backup - Using DigitalOcean Backups

on:
  schedule:
    - cron: '0 2 * * 0'  # Every Sunday at 2 AM UTC
  workflow_dispatch:

# Concurrency: Only one backup/restore operation at a time
concurrency:
  group: db-backup-restore
  cancel-in-progress: false

jobs:
  restore-from-do-backup:
    name: Restore Production Backup to Staging
    runs-on: ubuntu-latest

    steps:
    - name: ğŸ“¦ Install doctl (DigitalOcean CLI)
      run: |
        cd ~
        wget https://github.com/digitalocean/doctl/releases/download/v1.104.0/doctl-1.104.0-linux-amd64.tar.gz
        tar xf doctl-1.104.0-linux-amd64.tar.gz
        sudo mv doctl /usr/local/bin

    - name: ğŸ” Authenticate with DigitalOcean
      env:
        DO_API_TOKEN: ${{ secrets.DO_API_TOKEN }}
      run: |
        doctl auth init --access-token "$DO_API_TOKEN"

    - name: ğŸ” Verify Cluster ID and Token
      run: |
        echo "ğŸ” Verifying configuration..."
        echo "Cluster ID: ${{ secrets.PROD_DB_CLUSTER_ID }}"
        echo "Cluster ID length: ${#PROD_DB_CLUSTER_ID}"

        # List all databases to verify access
        echo "ğŸ“‹ Listing all databases..."
        doctl databases list
      env:
        PROD_DB_CLUSTER_ID: ${{ secrets.PROD_DB_CLUSTER_ID }}

    - name: ğŸ“‹ List Production Database Backups
      run: |
        echo "ğŸ“Š Listing available backups for cluster: ${{ secrets.PROD_DB_CLUSTER_ID }}"
        doctl databases backups list ${{ secrets.PROD_DB_CLUSTER_ID }} || echo "âš ï¸ Failed to list backups"

    - name: ğŸ”„ Trigger Production Database Backup
      run: |
        echo "ğŸ”„ Creating new production backup..."
        # DigitalOcean creates automatic daily backups
        # This step is optional as backups are automatic
        echo "âœ… Daily backups are automatic on DigitalOcean"

    - name: ğŸ“Š Get Latest Backup Information
      id: get_backup
      run: |
        echo "ğŸ“Š Getting latest production backup..."
        LATEST_BACKUP=$(doctl databases backups list ${{ secrets.PROD_DB_CLUSTER_ID }} --format CreatedAt,Size --no-header | head -n1)
        echo "Latest backup: $LATEST_BACKUP"
        echo "latest_backup=$LATEST_BACKUP" >> $GITHUB_OUTPUT

    - name: ğŸ“¥ Backup Production to Local File
      env:
        PROD_DB_URL: ${{ secrets.PRODUCTION_DB_URL }}
      run: |
        echo "ğŸ“¥ Creating local backup of production database..."

        # Install PostgreSQL 17 client to match server version
        sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
        wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
        sudo apt-get update

        # Remove old version and install PostgreSQL 17 client
        sudo apt-get remove -y postgresql-client postgresql-client-16 || true
        sudo apt-get install -y postgresql-client-17

        # Verify PostgreSQL version - use explicit path
        /usr/lib/postgresql/17/bin/pg_dump --version

        # Create backup file with timestamp
        BACKUP_FILE="prod_backup_$(date +%Y%m%d_%H%M%S).sql"

        # Use explicit path to pg_dump version 17
        /usr/lib/postgresql/17/bin/pg_dump "$PROD_DB_URL" \
          --no-owner \
          --no-privileges \
          --clean \
          --if-exists \
          -f "$BACKUP_FILE"

        BACKUP_SIZE=$(du -h "$BACKUP_FILE" | cut -f1)
        echo "âœ… Backup created: $BACKUP_FILE (Size: $BACKUP_SIZE)"
        echo "BACKUP_FILE=$BACKUP_FILE" >> $GITHUB_ENV

    - name: ğŸ—‘ï¸ Clean Staging Database
      env:
        STAGING_DB_URL: ${{ secrets.STAGING_DB_URL }}
      run: |
        echo "ğŸ—‘ï¸ Cleaning staging database..."

        /usr/lib/postgresql/17/bin/psql "$STAGING_DB_URL" -c "DROP SCHEMA public CASCADE; CREATE SCHEMA public;"

        echo "âœ… Staging database cleaned"

    - name: ğŸ“¤ Restore Backup to Staging
      env:
        STAGING_DB_URL: ${{ secrets.STAGING_DB_URL }}
      run: |
        echo "ğŸ“¤ Restoring production backup to staging..."

        /usr/lib/postgresql/17/bin/psql "$STAGING_DB_URL" < "$BACKUP_FILE"

        echo "âœ… Backup restored to staging"

    - name: ğŸ§ª Verify Staging Database
      env:
        STAGING_DB_URL: ${{ secrets.STAGING_DB_URL }}
      run: |
        echo "ğŸ§ª Verifying staging database..."

        TABLE_COUNT=$(/usr/lib/postgresql/17/bin/psql "$STAGING_DB_URL" -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';")

        echo "ğŸ“Š Tables in staging: $TABLE_COUNT"

        if [ "$TABLE_COUNT" -gt 0 ]; then
          echo "âœ… Staging database verified successfully"
        else
          echo "âš ï¸ Warning: No tables found in staging"
          exit 1
        fi

    - name: ğŸ§¹ Cleanup
      if: always()
      run: |
        rm -f prod_backup_*.sql
        echo "âœ… Cleanup completed"

    - name: ğŸ“§ Notification
      if: always()
      run: |
        if [ "${{ job.status }}" == "success" ]; then
          echo "ğŸ‰ Production database successfully backed up and restored to staging!"
          echo "ğŸ“… Backup date: $(date)"
          echo "ğŸ”„ Next scheduled backup: Next Sunday at 2:00 AM UTC"
          echo "ğŸ“Š Latest DO backup: ${{ steps.get_backup.outputs.latest_backup }}"
        else
          echo "âŒ Database backup/restore failed!"
          echo "ğŸ“… Failed at: $(date)"
          echo "âš ï¸ Please check logs"
        fi

    - name: ğŸ“¦ Upload Backup Artifact
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: production-db-backup-${{ github.run_number }}
        path: prod_backup_*.sql
        retention-days: 30
